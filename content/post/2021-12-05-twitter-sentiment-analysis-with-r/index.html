---
title: "Twitter Sentiment Analysis with R"
author: "Peter Boshe"
date: '2021-12-05'
slug: []
categories:
- R
- Educational
tags:
- Data Mining
- E.D.A
- Data Analysis
- Unguided project
- R programming
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
draft: false 
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>
<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/plotly-binding/plotly.js"></script>
<script src="{{< blogdown/postref >}}index_files/typedarray/typedarray.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/jquery/jquery.min.js"></script>
<link href="{{< blogdown/postref >}}index_files/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/crosstalk/js/crosstalk.min.js"></script>
<link href="{{< blogdown/postref >}}index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/plotly-main/plotly-latest.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/d3/d3.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/forceNetwork-binding/forceNetwork.js"></script>

<div id="TOC">
<ul>
<li><a href="#sentiment-analysis">Sentiment Analysis</a></li>
</ul>
</div>

<div id="sentiment-analysis" class="section level2">
<h2>Sentiment Analysis</h2>
<p>Sentiment analysis gives us insight into the things that automate mining of attitudes, opinions, views and emotions from text, speech, tweets and database sources. However, to fully explore the possibilities of this text analysis technique, we need data visualization tools to help organize the results. Visually representing the content of a text document is one of the most important tasks in the field of text mining.</p>
<p>However, there are some gaps between visualizing unstructured (text) data and structured data. Many text visualizations do not represent the text directly, they represent an output of a language model. In this post, we will use tweets extracted using Twitter API, store tweets as text data, classify opinions in text into categories like positive, or negative or neutral, create a function to calculate the score of each type of opinion in the text and try to explore and visualize as much as we can, using R libraries.</p>
<p>Tweets can be imported into R using Twitter API, then the text data has to be cleaned before analysis, for example removing emoticons, removing URLs, etc.</p>
<div id="set-up-environment" class="section level4">
<h4>Set up environment</h4>
<pre class="r"><code>library(rtweet)
library(twitteR)
library(ROAuth)
library(hms)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(rvest)
library(httr)</code></pre>
</div>
<div id="authorization-to-the-twitter-api" class="section level4">
<h4>Authorization to the twitter API</h4>
<ol style="list-style-type: decimal">
<li>using web-browser -rtweet</li>
</ol>
<pre class="r"><code>## store api keys (these are fake example values; replace with your own keys)
api_key &lt;- &quot;5xhi6DEejn9LFUugzOnJv0iQi&quot;
api_secret_key &lt;- &quot;vzNWoflXr7XhxMTSBXJ6DJAFhquJvxrPj29EnuW6VSyhBgq6tN&quot;
access_token &lt;- &quot;258913035-Q6qHwCQcrw4msCfzJxvHFT5EfEgyMnMTvXaFyCFx&quot;
access_token_secret &lt;- &quot;iG3nnZSPEmWdbrtoEJYczX1yqr6RooFcqIUCEQblo7Y6U&quot;

## authenticate via web browser
# token &lt;- create_token(
#   app = &quot;twitter-tea&quot;,
#   consumer_key = api_key,
#   consumer_secret = api_secret_key,
#   access_token = access_token,
#   access_secret = access_token_secret)

#The create_token() function should automatically save your token as an environment variable for you. So next time you start an R session [on the same machine], rtweet should automatically find your token. or use get_token() to locate</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>using access tokens (Direct Authentication) -twitteR</li>
</ol>
<pre class="r"><code>#Note: This will ask us permission for direct authentication, type &#39;1&#39; for yes:
setup_twitter_oauth(api_key,api_secret_key,access_token,access_token_secret)
## [1] &quot;Using direct authentication&quot;

#to be called during each session</code></pre>
</div>
<div id="extracting-global-warming-tweets" class="section level4">
<h4>Extracting Global Warming tweets</h4>
<pre class="r"><code># extracting 4000 tweets related to global warming topic
tweets &lt;- searchTwitter(&quot;#globalwarming&quot;, n=4000, lang=&quot;en&quot;)
n.tweet &lt;- length(tweets)

# convert tweets to a data frame
tweets.df &lt;- twListToDF(tweets)

tweets.txt &lt;- sapply(tweets, function(t)t$getText())
# Ignore graphical Parameters to avoid input errors
tweets.txt &lt;- str_replace_all(tweets.txt,&quot;[^[:graph:]]&quot;, &quot; &quot;)

## pre-processing text:
clean.text = function(x)
{
  # convert to lower case
  x = tolower(x)
  # remove rt
  x = gsub(&quot;rt&quot;, &quot;&quot;, x)
  # remove at
  x = gsub(&quot;@\\w+&quot;, &quot;&quot;, x)
  # remove punctuation
  x = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, x)
  # remove numbers
  x = gsub(&quot;[[:digit:]]&quot;, &quot;&quot;, x)
  # remove links http
  x = gsub(&quot;http\\w+&quot;, &quot;&quot;, x)
  # remove tabs
  x = gsub(&quot;[ |\t]{2,}&quot;, &quot;&quot;, x)
  # remove blank spaces at the beginning
  x = gsub(&quot;^ &quot;, &quot;&quot;, x)
  # remove blank spaces at the end
  x = gsub(&quot; $&quot;, &quot;&quot;, x)
  # some other cleaning text
  x = gsub(&#39;https://&#39;,&#39;&#39;,x)
  x = gsub(&#39;http://&#39;,&#39;&#39;,x)
  x = gsub(&#39;[^[:graph:]]&#39;, &#39; &#39;,x)
  x = gsub(&#39;[[:punct:]]&#39;, &#39;&#39;, x)
  x = gsub(&#39;[[:cntrl:]]&#39;, &#39;&#39;, x)
  x = gsub(&#39;\\d+&#39;, &#39;&#39;, x)
  x = str_replace_all(x,&quot;[^[:graph:]]&quot;, &quot; &quot;)
  return(x)
}

cleanText &lt;- clean.text(tweets.txt)
# remove empty results (if any)
idx &lt;- which(cleanText == &quot; &quot;)
cleanText &lt;- cleanText[cleanText != &quot; &quot;]

BRRR::skrrrahh(44)</code></pre>
</div>
<div id="frequency-of-tweets" class="section level4">
<h4>Frequency of Tweets</h4>
<pre class="r"><code>tweets.df %&lt;&gt;% 
  mutate(
    created = created %&gt;% 
      # Remove zeros.
      str_remove_all(pattern = &#39;\\+0000&#39;) %&gt;%
      # Parse date.
      parse_date_time(orders = &#39;%y-%m-%d %H%M%S&#39;)
  )

tweets.df %&lt;&gt;% 
  mutate(Created_At_Round = created%&gt;% round(units = &#39;hours&#39;) %&gt;% as.POSIXct())

tweets.df %&gt;% pull(created) %&gt;% min()
## [1] &quot;2021-12-04 03:55:59 UTC&quot;

tweets.df %&gt;% pull(created) %&gt;% max()
## [1] &quot;2021-12-09 02:39:20 UTC&quot;

plt &lt;- tweets.df %&gt;% 
  dplyr::count(Created_At_Round) %&gt;% 
  ggplot(mapping = aes(x = Created_At_Round, y = n)) +
  theme_light() +
  geom_line() +
  xlab(label = &#39;Date&#39;) +
  ylab(label = NULL) +
  ggtitle(label = &#39;Number of Tweets per Hour&#39;)

plt %&gt;% ggplotly()</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"x":[1638590400,1638594000,1638597600,1638601200,1638604800,1638608400,1638612000,1638615600,1638619200,1638622800,1638626400,1638630000,1638633600,1638637200,1638640800,1638644400,1638648000,1638651600,1638655200,1638658800,1638662400,1638666000,1638669600,1638673200,1638676800,1638680400,1638684000,1638687600,1638691200,1638694800,1638698400,1638702000,1638705600,1638709200,1638712800,1638716400,1638720000,1638723600,1638727200,1638730800,1638734400,1638738000,1638741600,1638745200,1638748800,1638752400,1638756000,1638759600,1638763200,1638766800,1638770400,1638774000,1638777600,1638781200,1638784800,1638788400,1638792000,1638795600,1638799200,1638802800,1638806400,1638810000,1638813600,1638817200,1638820800,1638824400,1638828000,1638831600,1638835200,1638838800,1638842400,1638846000,1638849600,1638853200,1638856800,1638860400,1638864000,1638867600,1638871200,1638874800,1638878400,1638882000,1638885600,1638889200,1638892800,1638896400,1638900000,1638903600,1638907200,1638910800,1638914400,1638918000,1638921600,1638925200,1638928800,1638932400,1638936000,1638939600,1638943200,1638946800,1638950400,1638954000,1638957600,1638961200,1638964800,1638968400,1638972000,1638975600,1638979200,1638982800,1638986400,1638990000,1638993600,1638997200,1639000800,1639004400,1639008000,1639011600,1639015200,1639018800],"y":[31,23,24,21,8,10,17,25,32,50,32,43,58,44,41,38,49,77,22,28,16,26,23,15,28,24,17,26,14,22,34,22,37,23,19,24,41,22,40,32,24,31,46,26,18,23,13,17,18,26,24,30,28,24,26,23,22,26,68,38,43,48,42,58,41,50,82,27,27,19,22,39,44,35,32,36,38,27,32,68,74,48,49,62,37,34,52,43,50,48,33,37,18,17,35,36,28,20,34,44,41,37,37,40,58,33,32,27,35,33,32,38,39,45,26,28,27,31,17,6],"text":["Created_At_Round: 2021-12-04 04:00:00<br />n: 31","Created_At_Round: 2021-12-04 05:00:00<br />n: 23","Created_At_Round: 2021-12-04 06:00:00<br />n: 24","Created_At_Round: 2021-12-04 07:00:00<br />n: 21","Created_At_Round: 2021-12-04 08:00:00<br />n:  8","Created_At_Round: 2021-12-04 09:00:00<br />n: 10","Created_At_Round: 2021-12-04 10:00:00<br />n: 17","Created_At_Round: 2021-12-04 11:00:00<br />n: 25","Created_At_Round: 2021-12-04 12:00:00<br />n: 32","Created_At_Round: 2021-12-04 13:00:00<br />n: 50","Created_At_Round: 2021-12-04 14:00:00<br />n: 32","Created_At_Round: 2021-12-04 15:00:00<br />n: 43","Created_At_Round: 2021-12-04 16:00:00<br />n: 58","Created_At_Round: 2021-12-04 17:00:00<br />n: 44","Created_At_Round: 2021-12-04 18:00:00<br />n: 41","Created_At_Round: 2021-12-04 19:00:00<br />n: 38","Created_At_Round: 2021-12-04 20:00:00<br />n: 49","Created_At_Round: 2021-12-04 21:00:00<br />n: 77","Created_At_Round: 2021-12-04 22:00:00<br />n: 22","Created_At_Round: 2021-12-04 23:00:00<br />n: 28","Created_At_Round: 2021-12-05 00:00:00<br />n: 16","Created_At_Round: 2021-12-05 01:00:00<br />n: 26","Created_At_Round: 2021-12-05 02:00:00<br />n: 23","Created_At_Round: 2021-12-05 03:00:00<br />n: 15","Created_At_Round: 2021-12-05 04:00:00<br />n: 28","Created_At_Round: 2021-12-05 05:00:00<br />n: 24","Created_At_Round: 2021-12-05 06:00:00<br />n: 17","Created_At_Round: 2021-12-05 07:00:00<br />n: 26","Created_At_Round: 2021-12-05 08:00:00<br />n: 14","Created_At_Round: 2021-12-05 09:00:00<br />n: 22","Created_At_Round: 2021-12-05 10:00:00<br />n: 34","Created_At_Round: 2021-12-05 11:00:00<br />n: 22","Created_At_Round: 2021-12-05 12:00:00<br />n: 37","Created_At_Round: 2021-12-05 13:00:00<br />n: 23","Created_At_Round: 2021-12-05 14:00:00<br />n: 19","Created_At_Round: 2021-12-05 15:00:00<br />n: 24","Created_At_Round: 2021-12-05 16:00:00<br />n: 41","Created_At_Round: 2021-12-05 17:00:00<br />n: 22","Created_At_Round: 2021-12-05 18:00:00<br />n: 40","Created_At_Round: 2021-12-05 19:00:00<br />n: 32","Created_At_Round: 2021-12-05 20:00:00<br />n: 24","Created_At_Round: 2021-12-05 21:00:00<br />n: 31","Created_At_Round: 2021-12-05 22:00:00<br />n: 46","Created_At_Round: 2021-12-05 23:00:00<br />n: 26","Created_At_Round: 2021-12-06 00:00:00<br />n: 18","Created_At_Round: 2021-12-06 01:00:00<br />n: 23","Created_At_Round: 2021-12-06 02:00:00<br />n: 13","Created_At_Round: 2021-12-06 03:00:00<br />n: 17","Created_At_Round: 2021-12-06 04:00:00<br />n: 18","Created_At_Round: 2021-12-06 05:00:00<br />n: 26","Created_At_Round: 2021-12-06 06:00:00<br />n: 24","Created_At_Round: 2021-12-06 07:00:00<br />n: 30","Created_At_Round: 2021-12-06 08:00:00<br />n: 28","Created_At_Round: 2021-12-06 09:00:00<br />n: 24","Created_At_Round: 2021-12-06 10:00:00<br />n: 26","Created_At_Round: 2021-12-06 11:00:00<br />n: 23","Created_At_Round: 2021-12-06 12:00:00<br />n: 22","Created_At_Round: 2021-12-06 13:00:00<br />n: 26","Created_At_Round: 2021-12-06 14:00:00<br />n: 68","Created_At_Round: 2021-12-06 15:00:00<br />n: 38","Created_At_Round: 2021-12-06 16:00:00<br />n: 43","Created_At_Round: 2021-12-06 17:00:00<br />n: 48","Created_At_Round: 2021-12-06 18:00:00<br />n: 42","Created_At_Round: 2021-12-06 19:00:00<br />n: 58","Created_At_Round: 2021-12-06 20:00:00<br />n: 41","Created_At_Round: 2021-12-06 21:00:00<br />n: 50","Created_At_Round: 2021-12-06 22:00:00<br />n: 82","Created_At_Round: 2021-12-06 23:00:00<br />n: 27","Created_At_Round: 2021-12-07 00:00:00<br />n: 27","Created_At_Round: 2021-12-07 01:00:00<br />n: 19","Created_At_Round: 2021-12-07 02:00:00<br />n: 22","Created_At_Round: 2021-12-07 03:00:00<br />n: 39","Created_At_Round: 2021-12-07 04:00:00<br />n: 44","Created_At_Round: 2021-12-07 05:00:00<br />n: 35","Created_At_Round: 2021-12-07 06:00:00<br />n: 32","Created_At_Round: 2021-12-07 07:00:00<br />n: 36","Created_At_Round: 2021-12-07 08:00:00<br />n: 38","Created_At_Round: 2021-12-07 09:00:00<br />n: 27","Created_At_Round: 2021-12-07 10:00:00<br />n: 32","Created_At_Round: 2021-12-07 11:00:00<br />n: 68","Created_At_Round: 2021-12-07 12:00:00<br />n: 74","Created_At_Round: 2021-12-07 13:00:00<br />n: 48","Created_At_Round: 2021-12-07 14:00:00<br />n: 49","Created_At_Round: 2021-12-07 15:00:00<br />n: 62","Created_At_Round: 2021-12-07 16:00:00<br />n: 37","Created_At_Round: 2021-12-07 17:00:00<br />n: 34","Created_At_Round: 2021-12-07 18:00:00<br />n: 52","Created_At_Round: 2021-12-07 19:00:00<br />n: 43","Created_At_Round: 2021-12-07 20:00:00<br />n: 50","Created_At_Round: 2021-12-07 21:00:00<br />n: 48","Created_At_Round: 2021-12-07 22:00:00<br />n: 33","Created_At_Round: 2021-12-07 23:00:00<br />n: 37","Created_At_Round: 2021-12-08 00:00:00<br />n: 18","Created_At_Round: 2021-12-08 01:00:00<br />n: 17","Created_At_Round: 2021-12-08 02:00:00<br />n: 35","Created_At_Round: 2021-12-08 03:00:00<br />n: 36","Created_At_Round: 2021-12-08 04:00:00<br />n: 28","Created_At_Round: 2021-12-08 05:00:00<br />n: 20","Created_At_Round: 2021-12-08 06:00:00<br />n: 34","Created_At_Round: 2021-12-08 07:00:00<br />n: 44","Created_At_Round: 2021-12-08 08:00:00<br />n: 41","Created_At_Round: 2021-12-08 09:00:00<br />n: 37","Created_At_Round: 2021-12-08 10:00:00<br />n: 37","Created_At_Round: 2021-12-08 11:00:00<br />n: 40","Created_At_Round: 2021-12-08 12:00:00<br />n: 58","Created_At_Round: 2021-12-08 13:00:00<br />n: 33","Created_At_Round: 2021-12-08 14:00:00<br />n: 32","Created_At_Round: 2021-12-08 15:00:00<br />n: 27","Created_At_Round: 2021-12-08 16:00:00<br />n: 35","Created_At_Round: 2021-12-08 17:00:00<br />n: 33","Created_At_Round: 2021-12-08 18:00:00<br />n: 32","Created_At_Round: 2021-12-08 19:00:00<br />n: 38","Created_At_Round: 2021-12-08 20:00:00<br />n: 39","Created_At_Round: 2021-12-08 21:00:00<br />n: 45","Created_At_Round: 2021-12-08 22:00:00<br />n: 26","Created_At_Round: 2021-12-08 23:00:00<br />n: 28","Created_At_Round: 2021-12-09 00:00:00<br />n: 27","Created_At_Round: 2021-12-09 01:00:00<br />n: 31","Created_At_Round: 2021-12-09 02:00:00<br />n: 17","Created_At_Round: 2021-12-09 03:00:00<br />n:  6"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":40.1826484018265,"l":22.648401826484},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"title":{"text":"Number of Tweets per Hour","font":{"color":"rgba(0,0,0,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[1638568980,1639040220],"tickmode":"array","ticktext":["Dec 05","Dec 07","Dec 09"],"tickvals":[1638662400,1638835200,1639008000],"categoryorder":"array","categoryarray":["Dec 05","Dec 07","Dec 09"],"nticks":null,"ticks":"outside","tickcolor":"rgba(179,179,179,1)","ticklen":3.65296803652968,"tickwidth":0.33208800332088,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(222,222,222,1)","gridwidth":0.33208800332088,"zeroline":false,"anchor":"y","title":{"text":"Date","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[2.2,85.8],"tickmode":"array","ticktext":["20","40","60","80"],"tickvals":[20,40,60,80],"categoryorder":"array","categoryarray":["20","40","60","80"],"nticks":null,"ticks":"outside","tickcolor":"rgba(179,179,179,1)","ticklen":3.65296803652968,"tickwidth":0.33208800332088,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(222,222,222,1)","gridwidth":0.33208800332088,"zeroline":false,"anchor":"x","title":{"text":"","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":"transparent","line":{"color":"rgba(179,179,179,1)","width":0.66417600664176,"linetype":"solid"},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"a58b47cbc6e51":{"x":{},"y":{},"type":"scatter"}},"cur_data":"a58b47cbc6e51","visdat":{"a58b47cbc6e51":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>

BRRR::skrrrahh(44)
</div>
<div id="estimating-sentiment-score" class="section level4">
<h4>Estimating Sentiment Score</h4>
<p>There are many resources describing methods to estimate sentiment. For the purpose of this tutorial, we will use a very simple algorithm which assigns sentiment score of the text by simply counting the number of occurrences of “positive” and “negative” words in a tweet.</p>
<p>Hu &amp; Liu have published an “Opinion Lexicon” that categorizes approximately 6,800 words as positive or negative, which can be downloaded from this link:</p>
<p><a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html" class="uri">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a></p>
<div id="loading-sentiment-word-lists" class="section level5">
<h5>Loading sentiment word lists</h5>
<pre class="r"><code>positive = scan(&#39;/home/pedro/Portfolio/web-tea/content/post/2021-12-05-twitter-sentiment-analysis-with-r/positive-words.txt&#39;, what = &#39;character&#39;, comment.char = &#39;;&#39;)
negative = scan(&#39;/home/pedro/Portfolio/web-tea/content/post/2021-12-05-twitter-sentiment-analysis-with-r/negative-words.txt&#39;, what = &#39;character&#39;, comment.char = &#39;;&#39;)
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,&#39;upgrade&#39;,&#39;Congrats&#39;,&#39;prizes&#39;,&#39;prize&#39;,&#39;thanks&#39;,&#39;thnx&#39;,
              &#39;Grt&#39;,&#39;gr8&#39;,&#39;plz&#39;,&#39;trending&#39;,&#39;recovering&#39;,&#39;brainstorm&#39;,&#39;leader&#39;)
neg.words = c(negative,&#39;wtf&#39;,&#39;wait&#39;,&#39;waiting&#39;,&#39;epicfail&#39;,&#39;Fight&#39;,&#39;fighting&#39;,
              &#39;arrest&#39;,&#39;no&#39;,&#39;not&#39;)

BRRR::skrrrahh(44)</code></pre>
</div>
<div id="sentiment-scoring-function" class="section level5">
<h5>Sentiment scoring function</h5>
<pre class="r"><code>
score.sentiment = function(sentences, pos.words, neg.words, .progress=&#39;none&#39;)
{
  require(plyr)
  require(stringr)
  
  # we are giving vector of sentences as input. 
  # plyr will handle a list or a vector as an &quot;l&quot; for us
  # we want a simple array of scores back, so we use &quot;l&quot; + &quot;a&quot; + &quot;ply&quot; = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # clean up sentences with R&#39;s regex-driven global substitute, gsub() function:
    sentence = gsub(&#39;https://&#39;,&#39;&#39;,sentence)
    sentence = gsub(&#39;http://&#39;,&#39;&#39;,sentence)
    sentence = gsub(&#39;[^[:graph:]]&#39;, &#39; &#39;,sentence)
    sentence = gsub(&#39;[[:punct:]]&#39;, &#39;&#39;, sentence)
    sentence = gsub(&#39;[[:cntrl:]]&#39;, &#39;&#39;, sentence)
    sentence = gsub(&#39;\\d+&#39;, &#39;&#39;, sentence)
    sentence = str_replace_all(sentence,&quot;[^[:graph:]]&quot;, &quot; &quot;)
    # and convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, &#39;\\s+&#39;)
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive &amp; negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

BRRR::skrrrahh(44)</code></pre>
</div>
<div id="calculating-the-sentiment-score" class="section level5">
<h5>Calculating the sentiment score</h5>
<pre class="r"><code>analysis &lt;- score.sentiment(cleanText, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)
## 
##   -4   -3   -2   -1    0    1    2    3 
##   13   45  208 1016 1998  587   94   39</code></pre>
</div>
</div>
<div id="histogram-of-sentiment-scores" class="section level4">
<h4>Histogram of sentiment scores</h4>
<pre class="r"><code>analysis %&gt;%
  ggplot(aes(x=score)) + 
  geom_histogram(binwidth = 1, fill = &quot;lightblue&quot;)+ 
  ylab(&quot;Frequency&quot;) + 
  xlab(&quot;sentiment score&quot;) +
  ggtitle(&quot;Distribution of Sentiment scores of the tweets&quot;) +
  ggeasy::easy_center_title()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Analysis: From the Histogram of Sentiment scores, we can see that around half of the tweets have sentiment score as zero i.e. Neutral and overall as expected, the distribution depicts negative sentiment in the tweets related to global warming, since it is a major issue of concern.</p>
</div>
<div id="barplot-of-sentiment-type" class="section level4">
<h4>Barplot of sentiment type</h4>
<pre class="r"><code>neutral &lt;- length(which(analysis$score == 0))
positive &lt;- length(which(analysis$score &gt; 0))
negative &lt;- length(which(analysis$score &lt; 0))
Sentiment &lt;- c(&quot;Positive&quot;,&quot;Neutral&quot;,&quot;Negative&quot;)
Count &lt;- c(positive,neutral,negative)
output &lt;- data.frame(Sentiment,Count)
output$Sentiment&lt;-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
  geom_bar(stat = &quot;identity&quot;, aes(fill = Sentiment))+
  ggtitle(&quot;Barplot of Sentiment type of 4000 tweets&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(44)</code></pre>
<p>Analysis: It is also clear from this barplot of sentiment type that around half of the tweets have sentiment score as zero i.e. Neutral and there are more negative sentiment tweets than that of positive sentiment. This barplot helps us to identify overall opinion of the people about global warming.</p>
</div>
<div id="wordcloud" class="section level4">
<h4>Wordcloud</h4>
<pre class="r"><code>text_corpus &lt;- Corpus(VectorSource(cleanText))
text_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))
## Warning in tm_map.SimpleCorpus(text_corpus, content_transformer(tolower)):
## transformation drops documents
text_corpus &lt;- tm_map(text_corpus, function(x)removeWords(x,stopwords(&quot;english&quot;)))
## Warning in tm_map.SimpleCorpus(text_corpus, function(x) removeWords(x,
## stopwords(&quot;english&quot;))): transformation drops documents
text_corpus &lt;- tm_map(text_corpus, removeWords, c(&quot;global&quot;,&quot;globalwarming&quot;))
## Warning in tm_map.SimpleCorpus(text_corpus, removeWords, c(&quot;global&quot;,
## &quot;globalwarming&quot;)): transformation drops documents
tdm &lt;- TermDocumentMatrix(text_corpus)
tdm &lt;- as.matrix(tdm)
tdm &lt;- sort(rowSums(tdm), decreasing = TRUE)
tdm &lt;- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
          colors=brewer.pal(8, &quot;Dark2&quot;), random.color = T, random.order = F)
## Warning in wordcloud(text_corpus, min.freq = 1, max.words = 100, scale =
## c(2.2, : uprootthesystem could not be fit on page. It will not be plotted.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(44)</code></pre>
<p>Analysis: Wordcloud helps us to visually understand the important terms frequently used in the tweets related to global warming, here for example, “climate change”, “environmental”, “temperature”, “emissions”, etc.</p>
</div>
<div id="word-frequency-plot" class="section level4">
<h4>Word Frequency plot</h4>
<pre class="r"><code>ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) + 
  geom_bar(stat=&quot;identity&quot;) +
  xlab(&quot;Terms&quot;) + 
  ylab(&quot;Count&quot;) + 
  coord_flip() +
  theme(axis.text=element_text(size=7)) +
  ggtitle(&#39;Most common word frequency plot&#39;) +
  ggeasy::easy_center_title()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(44)</code></pre>
<p>Analysis: we can infer that the most frequently used terms in the tweets related to global warming are, “climate”, “climatechange”, “since”, “biggest”, “hoax”, etc.</p>
</div>
<div id="network-analysis" class="section level4">
<h4>Network Analysis</h4>
<p>We are using a weighted network (graph) to describe how to encode and visualize text data. In this section we are counting pairwise relative occurrence of words.</p>
<div id="bigram-analysis-and-network-definition" class="section level5">
<h5>Bigram analysis and Network definition</h5>
<p>*Bigram counts pairwise occurrences of words which appear together in the text.</p>
<pre class="r"><code>#bigram
bi.gram.words &lt;- tweets.df %&gt;% 
  unnest_tokens(
    input = text, 
    output = bigram, 
    token = &#39;ngrams&#39;, 
    n = 2
  ) %&gt;% 
  filter(! is.na(bigram))

bi.gram.words %&gt;% 
  select(bigram) %&gt;% 
  head(10)
##                 bigram
## 1      rt officialmbtm
## 2  officialmbtm what&#39;s
## 3           what&#39;s the
## 4           the secret
## 5            secret of
## 6              of life
## 7            life more
## 8            more here
## 9           here https
## 10          https t.co


extra.stop.words &lt;- c(&#39;https&#39;)
stopwords.df &lt;- tibble(
  word = c(stopwords(kind = &#39;es&#39;),
           stopwords(kind = &#39;en&#39;),
           extra.stop.words)
)

BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>*Next, we filter for stop words and remove white spaces.</p>
<pre class="r"><code>bi.gram.words %&lt;&gt;% 
  separate(col = bigram, into = c(&#39;word1&#39;, &#39;word2&#39;), sep = &#39; &#39;) %&gt;% 
  filter(! word1 %in% stopwords.df$word) %&gt;% 
  filter(! word2 %in% stopwords.df$word) %&gt;% 
  filter(! is.na(word1)) %&gt;% 
  filter(! is.na(word2)) </code></pre>
<p>*Finally, we group and count by bigram.</p>
<pre class="r"><code>bi.gram.count &lt;- bi.gram.words %&gt;% 
  dplyr::count(word1, word2, sort = TRUE) %&gt;% 
  dplyr::rename(weight = n)

bi.gram.count %&gt;% head()
##           word1         word2 weight
## 1       climate        change    261
## 2  globalcrisis    time4truth    252
## 3 globalwarming climatechange    236
## 4  catastrophic globalwarming    227
## 5       causing  catastrophic    227
## 6       current           msm    227

BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>Let us plot the distribution of the weightvalues:</p>
<pre class="r"><code>bi.gram.count %&gt;% 
  ggplot(mapping = aes(x = weight)) +
  theme_light() +
  geom_histogram() +
  labs(title = &quot;Bigram Weight Distribution&quot;)
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:</p>
<pre class="r"><code>bi.gram.count %&gt;% 
  mutate(weight = log(weight + 1)) %&gt;% 
  ggplot(mapping = aes(x = weight)) +
  theme_light() +
  geom_histogram() +
  labs(title = &quot;Bigram log-Weight Distribution&quot;)
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>In order to define weighted network from a bigram count we used the following structure.</p>
<p><em>Each word is going to represent a node.
</em>Two words are going to be connected if they appear as a bigram.
*The weight of an edge is the number of times the bigram appears in the corpus.</p>
</div>
<div id="network-visualization" class="section level5">
<h5>Network visualization</h5>
<pre class="r"><code>threshold &lt;- 50

# For visualization purposes we scale by a global factor. 
ScaleWeight &lt;- function(x, lambda) {
  x / lambda
}

network &lt;-  bi.gram.count %&gt;%
  filter(weight &gt; threshold) %&gt;%
  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %&gt;% 
  graph_from_data_frame(directed = FALSE)

plot(
  network, 
  vertex.size = 1,
  vertex.label.color = &#39;black&#39;, 
  vertex.label.cex = 0.7, 
  vertex.label.dist = 1,
  edge.color = &#39;gray&#39;, 
  main = &#39;Bigram Count Network&#39;, 
  sub = glue(&#39;Weight Threshold: {threshold}&#39;), 
  alpha = 50
)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>We can even improve the representation by setting the sizes of the nodes and the edges by the degree and weight respectively.</p>
<pre class="r"><code>V(network)$degree &lt;- strength(graph = network)

# Compute the weight shares.
E(network)$width &lt;- E(network)$weight/max(E(network)$weight)

plot(
  network, 
  vertex.color = &#39;lightblue&#39;,
  # Scale node size by degree.
  vertex.size = 2*V(network)$degree,
  vertex.label.color = &#39;black&#39;, 
  vertex.label.cex = 0.6, 
  vertex.label.dist = 1.6,
  edge.color = &#39;gray&#39;, 
  # Set edge width proportional to the weight relative value.
  edge.width = 3*E(network)$width ,
  main = &#39;Bigram Count Network&#39;, 
  sub = glue(&#39;Weight Threshold: {threshold}&#39;), 
  alpha = 50
)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
BRRR::skrrrahh(&quot;44&quot;)
## Warning in BRRR::skrrrahh(&quot;44&quot;): &quot;44&quot; is not a valid sound nor path.</code></pre>
<p>We can go a step further and make our visualization more dynamic using the networkD3 library.</p>
<pre class="r"><code>threshold &lt;- 50

network &lt;-  bi.gram.count %&gt;%
  filter(weight &gt; threshold) %&gt;%
  graph_from_data_frame(directed = FALSE)

# Store the degree.
V(network)$degree &lt;- strength(graph = network)
# Compute the weight shares.
E(network)$width &lt;- E(network)$weight/max(E(network)$weight)

# Create networkD3 object.
network.D3 &lt;- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %&lt;&gt;% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %&lt;&gt;% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width &lt;- 10*E(network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = &#39;source&#39;, 
  Target = &#39;target&#39;,
  NodeID = &#39;name&#39;,
  Group = &#39;Group&#39;, 
  opacity = 0.9,
  Value = &#39;Width&#39;,
  Nodesize = &#39;Degree&#39;, 
  # We input a JavaScript function.
  linkWidth = JS(&quot;function(d) { return Math.sqrt(d.value); }&quot;), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)</code></pre>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="forceNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"links":{"source":[27,1,24,10,28,2,19,25,2,3,0,32,2,21,2,8,2,24,11,1,9,21,13,10,9,9,34,9,9,39,25,11,5,6,32,9,14,16,15,14,35,9,19,37,29,7,12,1,20,36,10,2],"target":[33,33,37,30,30,23,23,26,3,4,15,49,16,32,32,32,32,47,27,27,50,31,31,12,46,41,51,45,35,55,48,20,6,7,39,56,17,17,18,18,52,54,44,38,38,8,43,13,22,53,42,40],"value":[10,9.6551724137931,9.04214559386973,8.69731800766283,8.69731800766283,8.69731800766283,8.69731800766283,8.69731800766283,8.69731800766283,8.65900383141762,8.65900383141762,8.08429118773946,6.20689655172414,5.55555555555556,5.47892720306513,4.59770114942529,4.2911877394636,4.2911877394636,4.2911877394636,4.2911877394636,4.2911877394636,4.2911877394636,4.25287356321839,4.13793103448276,3.79310344827586,3.75478927203065,3.44827586206897,3.33333333333333,3.25670498084291,3.10344827586207,3.10344827586207,3.10344827586207,3.10344827586207,3.10344827586207,3.10344827586207,2.98850574712644,2.95019157088123,2.79693486590038,2.56704980842912,2.56704980842912,2.52873563218391,2.52873563218391,2.45210727969349,2.33716475095785,2.1455938697318,2.1455938697318,2.1455938697318,2.10727969348659,2.10727969348659,2.10727969348659,1.99233716475096,1.99233716475096],"colour":["#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666","#666"]},"nodes":{"name":["climate","globalcrisis","globalwarming","catastrophic","causing","current","msm","narrative","straight","rt","global","online","crisis","time4truth","avoid","change","climateactionnow","plastics","please","t.co","international","climatecrisis","truth","aqtl0bico4","approx","average","c","conference","part","temperature","2","creativesociety","climatechange","04.12.2021","will","graphsandcharts","slowly","1975","since","nytimes","whi","ellymelly","warming","time","sfimg5a2oe","genius_mindsss","drgem2015","climatedata","increase","climateaction","creativescty","gain","post","turn","rebeccah2030","guardian","officialmbtm"],"group":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"nodesize":[2.61,3.96,9.5,4.54,2.27,2.27,4.54,4.54,4.54,6.66,4.33,2.7,2.88,3.72,2.24,3.73,2.24,2.24,2.24,1.99,1.98,1.71,0.9,1.72,1.36,1.62,0.81,3.21,0.81,0.81,1.58,1.93,7.53,1.47,0.66,1.12,0.56,1.1,1.36,1.13,2.26,2.26,2.11,1.43,1.12,1.11,0.99,0.81,0.81,0.67,0.67,0.66,0.56,0.56,0.55,0.52,0.52]},"options":{"NodeID":"name","Group":"Group","colourScale":"d3.scaleOrdinal(d3.schemeCategory20);","fontSize":12,"fontFamily":"serif","clickTextSize":30,"linkDistance":50,"linkWidth":"function(d) { return Math.sqrt(d.value); }","charge":-30,"opacity":0.9,"zoom":true,"legend":false,"arrows":false,"nodesize":true,"radiusCalculation":" Math.sqrt(d.nodesize)+6","bounded":false,"opacityNoHover":1,"clickAction":null}},"evals":[],"jsHooks":[]}</script>

BRRR::skrrrahh("44")
## Warning in BRRR::skrrrahh("44"): "44" is not a valid sound nor path.
<p>In this blog, we explored how to extract data and insights from Twitter. We presented how to clean text data and perform sentiment analysis. Secondly, we saw how pairwise word counts give information about the relations of the input text. Lastly, we studied how to use networks to represent bigram count measures.</p>
</div>
</div>
<div id="references" class="section level4">
<h4>References</h4>
<p>Bing Liu, Minqing Hu and Junsheng Cheng. “Opinion Observer: Analyzing and Comparing Opinions on the Web.” Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.</p>
</div>
</div>
